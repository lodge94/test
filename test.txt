Since Black and Scholes (1973, henceforth BS) and Merton (1973) originally developed their option valuation formulas, researchers have developed option valuation models that incorporate stochastic volatility (see Heston (1993) and the references therein). The two types of volatility models have been continuous-time stochastic volatility models and discrete-time Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models. A related class of volatility models is the implied binomial tree or the deterministic volatility models of Derman and Kani (1994), Dupire (1994), and Rubinstein (1994) in which the spot volatility is a function of the culTent asset price and time only. This paper presents an option formula for a stochastic volatility model with Generalized Autoregressive Conditional Heteroskedasticity (GARCH). The new formula describes option values as functions of the current spot price and the observed path of historical spot prices. It captures both the stochastic nature of volatility and correlation between volatility and spot returns. On a daily frequency the model is numerically close to the continuous-time stochas- tic volatility model of Heston (1993), but much easier to apply with available data. Our empirical analysis on S&P 500 index options shows that the out-of- sample valuation errors from the GARCH model are much lower than those from other models, including heuristic rules that are used by market makers to fit to the variations in implied volatilities across strike prices and matu- rities. The GARCH model successfully predicts out-of-sample option prices because it exploits the correlation of volatility with the path of stock returns. In addition to improving the prediction of volatility, the correlation parameter induces strike price and maturity patterns across option values such as the pronounced smirk in implied volatilities in the index options market. Continuous-time stochastic volatility models example Heston (1993)] are difficult to implement and test. Although these models assume that volatil- ity is observable, it is impossible to exactly filter a volatility variable from discrete observations of spot asset prices in a continuous-time stochastic volatility model. Consequently it is not possible to compute out-of-sample options valuation errors from the history of asset returns. Also the unobserv- ability of volatility implies that one has to use implied volatilities computed from option prices to value other options. Holding the model parameters con- stant through time in Bates (1996, 1999) and Nandi (1998)], this approach requires estimating numerous implied volatilities from options records, one for every date and is computationally very burdensome in a long time series of options records. Another alternative is to estimate all parameters (includ- ing volatility) daily from the cross-section of observed option prices as in Bakshi, Cao and Chen (1997) or directly using the BS implied volatility from a particular option/options as a proxy for the unobserved spot volatility as in Knoch (1992). However, using implied volatilities to value an option requires the use of other contemporaneous options that may not always be feasible if one does not have reliable option prices such as in cases of thinly traded or illiquid markets. In contrast to the continuous-time models, GARCH models have the inher- ent advantage that volatility is readily observable from the history of asset prices. As a result, a GARCH option model allows one to value an option using spot volatilities computed directly from the history of asset returns without necessarily using the implied volatilities inferred from other con- temporaneous options. Thus it is possible to value an option solely on the basis of observables because the parameters of the valuation formula can be readily estimated from the discrete observations of asset prices. If a closed- form solution were available, a GARCH model would enable one to readily combine the cross-sectional information in options with the information in the time series of the underlying asset. Since volatility is a readily computed function of the history of asset prices, only a finite number of parameters need to be estimated irrespective of the length of the time series, thus con- siderably simplifying the estimation procedure. Unfortunately, existing GARCH models do not have closed-form solu- tions for option values. These models are typically solved by simulation and Mustafa (1992), Amin and Ng (1993), Duan (1995)] that can be slow and computationally intensive for empirical work. More recently, Duan, Gauthier and Simonato (1999) provide a series approximation and Ritchken and Trevor (1999) provide a lattice approximation to value American options for GARCH processes with single lags in the variance dynamics. In contrast, this paper develops a closed-form solution for European option values (and hedge ratios) in a GARCH model. The model allows for multiple lags in the time series dynamics of the variance process and also allows for colTelation between returns of the spot asset and variance. The single lag version of the model reconciles the discrete-time GARCH approach with the continuous- time stochastic volatility approach to option valuation by including Heston's (1993) closed-form stochastic volatility model as a continuous-time limit. This generalizes the Black-Scholes and Merton approach to option valuation because it is possible to value options by the absence of arbitrage only in the continuous-time limit, even though volatility is path dependent. In the BS model option values are functions of the current spot asset price, while in the GARCH model option values are functions of cuffent and lagged spot prices. Except for this difference the models are operationally similar. We test the empirical implications of our GARCH model in the S&P 500 index options market. As a benchmark model we choose the ad hoc BS model of Dumas, Fleming and Whaley (1998, henceforth DFW) that has the flexibility of fitting to the strike and term structure of observed implied volatilities by using a separate implied volatility for each option. It is found that the GARCH model has smaller valuation errors (out-of-sample) than the ad hoc BS model even though the ad hoc model is updated every period. In contrast, the parameters of the GARCH model are held constant over a sample period and variance is filtered from the history of asset prices. When we update the parameters of the GARCH model every period, the out-of- sample prediction errors decrease even further and substantially. Also the out-of-sample results remain essentially unchanged if we use the S&P 500 futures to filter the spot variance for our GARCH model instead of the S&P 500 cash index. Our out-of-sample valuation results stand in contrast to previous empiri- cal tests of the implied binomial tree/deterministic volatility models. In these tests DFW (1998) found that the ad hoc BS model dominated the determin- istic volatility models in terms of out-of-sample options valuation errors in the S&P 500 index options market. Most of the options valuation improve- ments by the GARCH model are seen to result from its ability to simultane- ously capture the path dependence in volatility and the negative correlation of volatility with index returns. This negative correlation allows the model to quickly adapt to changes in volatility associated with changes in the mar- ket levels. Also the negative correlation generates a negative skewness in the risk-neutral distribution of the S&P 500 index return. This is associated with the strike price and maturity specific biases in the index options market. Section 1 describes the GARCH process and presents the option for- mula. Section 2 applies it to the S&P500 index option data, Section 3 reports the in-sample and out-of-sample results, while Section 4 concludes. Appendix A contains detailed calculations and derivations of the option for- mula while Appendix B contains the calculations regarding the convergence of the GARCH model to its continuous-time limit.

Model calibration can be formulated as an inverse problem, where, based on observed output results, the input parameters need to be inferred. Previous work on solving inverse problems includes research on adjoint optimization methods (Deng et al., 2008; Bouchouev and Isakov, 1997)), Bayesian methods (Kennedy and O'Hagan, 2001; Cont, 2019), and sparsity regularization (Daubechies et al., 2004).
In a financial context, e.g., in the pricing and risk management of financial derivative contracts, asset model calibration aims at recovering the model parameters of the underlying stochastic differential equations (SDEs) from observed market data. In other words, in the case of stocks and financial options, the calibration aims to determine the stock model parameters such that heavily
traded, liquid option prices can be recovered by the mathematical model. The calibrated asset models are subsequently used to either determine a suitable price for over-the-counter (OTC) exotic financial derivatives products, or for hedging and risk management purposes.
Calibrating financial models is a critical subtask within finance, and may need to be performed numerous times every day. Relevant issues in this context include accuracy, speed and robustness of the calibration. Real-time pricing and risk management require a fast and accurate calibration process. Repeatedly computing the values using mathematical models and at the same time fitting the parameters may be a computationally heavy burden, especially when dealing with multi-dimensional asset price models. The calibration problem is
furthermore not necessarily a convex optimization problem, and it often gives rise to multiple local minima. A generic, robust calibration framework may be based on a global optimization technique in combination with a highly efficient pricing method, in a parallel computing environment. To meet these requirements, we will employ the machine learning technology and develop an artificial neural network (ANN) solution method for a generic calibration framework.
The proposed ANN-based framework comprises three phases, i.e., training, prediction and calibration. During the training phase, the hidden layer parameters of the ANNs are optimized by means of supervised learning. This training phase builds a mapping between the model parameters and the output of interest. During the prediction phase, the hidden layers are kept unchanged (frozen) to compute the output quantities (e.g.,option prices) given various input parameters
of the asset price model. The prediction phase can also be used to evaluate the model performance (namely testing). Together these steps are called the
forward pass. Finally, during the calibration phase, given the observed output data (e.g., market option prices), the original input layer becomes a learnable layer again, whereas all previously learned hidden layers are kept fixed. This latter
stage is also called the backward pass. The overall calibration framework we name CaNN (Calibration Neural Network) here. The CaNN inverts the already
trained neural network conditional on certain known input. There are several interesting aspects to the proposed approach. First of
all, the machine learning approach may significantly accelerate classical option pricing techniques, particularly when involved asset price models are of interest. Recently there has been increasing interest in applying machine-learning techniques for fast pricing and calibration, see (Liu et al., 2019; Poggio et al., 2017; Spiegeleer et al., 2018; Horvath et al., 2019; Dimitroff et al., 2018; Hernandez,
2016; Hirsa et al., 2019). For example, the paper (Spiegeleer et al., 2018) used Gaussian process regression methods for derivative pricing. Other work, including this paper, employs artificial neural networks to learn the solution of the financial SDE system (Liu et al., 2019; Horvath et al., 2019; Hirsa et al., 2019), that do not suffer much from the curse of dimensionality.
Secondly, there is inherent parallelism in our ANN approach, so we will also take advantage of modern processing units (like GPUs). The paper (Horvath et al., 2019) also presented a neural network-based method to compute and
calibrate rough volatility models. Our CaNN however incorporates a parallel global search method for calibration. Moreover, the CaNN is a generic ANNbased framework, and views the three phases, training/prediction/calibration, as a whole, the difference between them being just to change the learnable units. Furthermore, the proposed ANN approach can handle a exible number of input market data. In other papers, like (Hernandez, 2016), (Dimitrofi et al., 2018), the number of input data had to be fixed in order to fit the employed Convolutional Neural Networks.
Calibrating financial models often gives rise to non-convex optimization problems, for which local optimization algorithms may have convergence issues.
A local optimization technique is generally relatively cheap and fast, but a key factor is to choose an accurate initial guess. Otherwise it may fail to converge and get stuck in a local minimum. The authors in (Gilli and Schumann, 2012) vary two parameters of the Heston model (keeping the other parameters unchanged), and show that the objective function exhibits multiple local minima.
Also in (Homescu, 2011) it is stated that multiple local minimal are common for calibration in the FX and commodities markets.
To address robustness, global optimizers are popular to calibrate financial
models, like the Differential Evolution (DE) technique, Particle Swarm optimization
and Simulated Annealing, as their convergence does not rely on speci
fic initial values. DE has been used to calibrate financial models (Vollrath
and Wendland, 2009; Gilli and Schumann, 2012) and to train neural networks
(Slowik and Bialko, 2008). Parallel computing may help to solve calibration
problems with global optimization within reasonable time.
The contributions of this paper are three-fold. First, we design a generic
ANN-based framework for calibration. Apart from data generators, all the components
and tasks are implemented on a unified computing platform. Second,
a parallel global searcher is adopted based on a population-based optimization
algorithm (here DE), an approach that fits well within the ANN-based calibration
framework. Both the forward and backward passes run in parallel, tackling
the computational bottleneck of global optimization and making the calibration
time reasonable, even in the case of employing a large neural network. Third,
the key components are robust and stable: using a robust data generator in combination
and the global optimization technique makes sure that the ANN-based
calibration method does not get stuck in local minima.
The rest of the paper is organized as follows. In Section 2, the Heston and
Bates stochastic volatility models and their calibration requirements are brief
introduced. In Section 3, artificial neural networks are introduced as function
approximators, in the context of parametric financial models. In this section,
a generic machine learning framework for model calibration to find the global
solution is presented as well. In Section 4, numerical experiments are presented
to demonstrate the performance of the proposed calibration framework. Some
details of the employed COS option pricing method are given in the appendix.

This paper provides a review of the most significant volatility models and their related
option pricing models, where we survey the development from constant up to stochastic
volatility. We define volatility, the volatility types and study the empirical characteristics
e.g., leverage effect. We discuss the key attributes of each volatility modelling method,
explaining how they capture theoretical and empirical characteristics of implied and
realised volatility e.g., time scale variance. We also discuss less commonly known
models.
The study of volatility has become a significant area of research within financial
mathematics. Firstly, volatility helps us understand price dynamics since it is one of the
key variables in a stochastic differential equation governing an asset price. Secondly,
volatility is the only variable in the Black-Scholes option pricing equation that is
unobservable, hence, the ability to model volatility is crucial to option pricing.
Thirdly, volatility is a crucial factor in a wide range of research areas. For example,
contagion effects involve the ‘transmission’ of volatility from one country to another
(Baur, 2003). Volatility can explain extreme events as Blake (1990) explains that the
October 1987 crash could have resulted from volatility changes.
Finally, volatility has a wide range of industrial applications from pricing exotic
derivatives to asset pricing models (Rebonato, 2004). Shiller (1989) argues the market’s
volatility dynamics can be applied to macroeconomic variables, particularly as the stock
market is a well known leading indicator of the economy. Schiller (1981) also claims
volatility can be used as a measure of market efficiency.
Option pricing in itself has become an important research area. Research interest in
options pricing began with the Black-Scholes option pricing paper (Black and Scholes,
1973); since then the derivatives market has grown into a multi-trillion dollar industry
(Stout, 1999). Options have become important to industry, particularly as they can be
used to hedge out risk. In fact, in many situations it is more attractive to speculators and
hedgers to trade an option rather than an underlying due to the limited loss. Additionally,
option trading can normally be executed on a far higher level of leverage compared to
trading stocks, therefore offering potentially higher returns for the same initial deposit.
The outline of the paper is as follows. Firstly, we review basic financial mathematics
theory, which is essential for the study of volatility modelling and option pricing. Next,
we introduce the differing types of volatility and discuss their empirical behaviour e.g.,
leverage effect. We then discuss the key models of volatility and their associated option
pricing methods. We finally end with a conclusion.

Recent progress in the field of artificial intelligence (AI), machine learning (ML) and also in computer
industry resulted in the ongoing boom of using these techniques as applied to solving complex tasks in both
science and industry. Same is, of course, true for the financial industry and mathematical finance. However,
as mentioned in Statt (2018), "...how fast the industry is moving, and to what end, is typically measured
not just by actual product advancements and research milestones, but also by the prognostications and
voiced concerns of AI leaders, futurists, academics, economists, and policymakers. AI is going to change
the world — but how and when are still open questions".
In this paper we consider a classical problem of mathematical finance - calibration of option pricing
models to market data, as it was recently drawn some attention of the financial society in the context of deep learning (DL) and artificial neural networks (ANN)1. Among various papers, we mention Horvath
et al. (2019); Liu et al. (2019), and also references therein. In short, the main idea of these papers is as
follows. As known, given some asset pricing model the classical calibration problem consists in finding
models parameters optimal in a sense that they minimize the difference (in some norm) between the prices
predicted by the model and provided by the market. In this paper for the sake of certainty we consider
just the option pricing models. The idea of applying DL techniques to this problem is inspired by the
fact that computing model option prices could be slow, so the whole calibration is slow. Instead, the DL
calibration assumes this problem to be solved in two steps. The first step is to replace a slow pricer with
its approximator by using a ANN. This ANN is trained with some in-sample set of data, so the weights of
the ANN become known after this step is complete. Then a standard calibration is used where the model
pricer is replaced with the trained ANN constructed at the previous step.
The advantage of this approach is that for the given model the approximating ANN could be trained
offline just once, and then it could be used with any market data for the online calibration. Therefore, in
the above cited papers the authors claim significant acceleration of the calibration process which could
achieve several orders of magnitude (when doing such a comparison for the DL calibration, the offline
time of training the ANN is ignored).
This approach seems to be attractive and efficient while still requires addressing various technical
problems. A typical construction and training of the ANN is described in detail in Horvath et al. (2019).
For the general theory of using DL for asset pricing see, e.g., an extensive presentation Elouerkhaoui (2019)
and references therein. However, aside of these technical details (which, certainly are very important),
the approach advocated in the referenced literature has some internal pitfalls which we further discuss in
this paper. Some of them are as follows. First, in our opinion the second step of the calibration process
(running the global optimizer) could be eliminated as the same result could be achieved when training
the ANN at the first step. This requires some more delicate consideration which we provide in the paper.
Second, the ANN prices by default could not guarantee no-arbitrage, neither in-sample nor our-of sample.
This is a more serious problem that requires special attention. Finally, as option Greeks are as well
important as the prices themselves, one need to make sure that the ANN prices are at least C2, which
again requires a special consideration. However, the current approaches don’t take these problems into
account.
Therefore, the ultimate goal of this paper is to investigate these problems in more detail and provide
some (perhaps only partial) solutions.

The thesis consists of three focus on two broad topics, applying machine learning in finance and extracting implied information from options. I combine the data-driven approach from the machine learning community and economic theory from the finance community to design a deep neural network to estimate the implied volatility surface. I propose a framework on how to design a deep learning network combined with economic assumptions. This approach can be used for any modelling purpose rather than just modelling the implied volatility surface. The empirical experiments show that our model has better performance, insample and out-of-sample, than the benchmark model. is a second example of applying machine learning in finance.
Yang et al. proposes a gated neural network for pricing European call options. They integrate assumptions of a valid call option surface from classical mathematical finance literature into neural network as inductive bias. Yang et al. is rewritten in this the general framework introduced in. The neural networkmodel outperforms existing learning-based and some mathematical finance models, and comes with guarantees about the economic rationality of its outputs. I provide a solution to the following question. Is there any flexible implementation framework to derive the conditional risk neutral density of any arbitrary period of return and calculate corresponding statistics, namely, implied variance, implied skewness and implied kurtosis from option prices? I solve this problem by proposing a framework combining implied volatility surface and Automatic Di↵erentiation 1981, Neidinger, 2010, Griewank and Walther, 2008, Baydin et al., 2015]. The framework exhibits the following features.
• It is flexible to change the implied volatility surface model without changing the main programming code. This is achieved by introducing Automatic Di↵erentiation. Construction of an arbitrage-free implied volatility surface is a well-studied research area and new models are still coming out. Switching between these models to select the best one is clearly an advantage.
• The conditional risk neutral density of arbitrary period of return, for example, weekly, monthly or quarterly, can be derived from the framework on each trading day. Furthermore, daily time series of implied variance, skewness and kurtosis with respect to the return distribution is readily computed.
With the help of the above framework to construct the implied information, I test the hypothesis whether predictability of variance risk premium on S&P500 index return still holds for shorter frequency, say weekly, biweekly or triweekly. This question will be of interest to practitioners as if the predictability exists, short term trading strategy can be designed. Furthermore, if the predictability exits, this will be an example demonstrating that the short term options, which are ignored by academic are worth studying et al., 2017]. Furthermore, I also explore an interesting question as to whether di↵erent weekdays, which are used to define weekly return and weekly variance risk premium, will influence the predictive power.
From the regression results, we conclude that the weekly frequency results are more informative than biweekly and triweekly as only Wednesday is not significant if the alpha level is 5%. For biweekly and triweekly results, only two out of five days are significant and these weekdays are not the same. This raises an interesting question as to whether there exists a weekday e↵ect which influences the predictive power of the short-term variance risk premium. The monthly and quarterly results are consistent with the literature et al., 2009, Carr and Wu, 2009, Bollerslev et al., 2011, Bondarenko, 2014, Xiao and Zhou, 2015]. This analysis allows us to conclude that the framework is useful in constructing the implied information for prediction purposes.

In recent years machine learning has gained a lot of popularity within all areas of scientific research, with finance being no exception. The need to handle large amounts of data and heavy computations in the financial industry has been growing since the late 20th century, with banks seeing more and more regulatory requirements for financial derivatives as a consequence of the financial crisis of 2007- 2008. This demands fast pricing and risk management tools for calculating on the fly prices and risk sensitivities, which the newest machine learning models aims to provide. Even though machine learning models have been growing in popularity, so has the scepticism of these models. Especially models with complex structures, such as artificial neural networks that has been touted as black boxes, as they are generally not as well understood as traditional statistical models, making it hard to explain why they actually work, but are usually used because they work really well, when applied correctly. This is also the reason for the rise in popularity in the field of explainable AI. In this thesis we want to shine light on an example, where the machine learning model, and why it works, is actually very well understood, and where its practical purpose makes a huge difference, namely on the topic of pricing and risk management of financial derivatives.

In 2018, the Chicago Board Options Exchange reported that over $1 quadrillion worth of options were traded in the US . Option contracts are a financial derivative that represents the right, but not the obligation, to buy (call) or sell (put) a particular security on (European type) or before (American type) an expiration date. A subset of these instruments were first priced with the Black-Scholes formula where the premium of a European call option without dividends is given. The inputs are S the underlying price, X the exercise price, T the
annualized fraction of time until expiration, r the risk free interest rate, the standard deviation of stock price returns, which cannot be directly observed. This model was derived by assuming stock prices are continuous and follow geometric Brownian motion, r are constant over the option life, and that the market is frictionless (no transaction costs, short squeeze, taxes, etc.).
However, even controlled for assumptions, Black-Scholes mismatches empirical findings and fails to explain the volatility surface. This precipitated more mathematically complex approaches such as the Heston model or jump-diffusion processes, as well as Monte Carlo methods to leverage advances in computing .
Alternatively, we can view an option as a function of the contract terms X and T, as well as information on the prevailing financial state S, r. This provides a foundation on which to build a computational model that can evade assumptions about financial mechanics and learn from historical options data. We hope to build a neural network model and compare its performance to the seminal Black-Scholes model. At the most basic level, our inputs are the terms of the option contract and the price of the underlying at transaction time and our output would be the price of the option.
We will explore models that take advantage of previous state information such as recurrent neural networks (RNNs) or volatility estimates and risk free rate. Malliaris and Salchenberger first studied a neural network approach to estimate the close price
of S&P 100 options using transaction data from the first six months of 1990. They supplemented the contract data with the option premium and underlying price for the day prior, as well as historical realized volatility. They further divided their training set to separately forecast the price of in the money and out of the money options, which may lead to overfitting and disagrees with the practically continuous property of stock prices. They constructed a neural network with a hidden layer of four nodes and one output node, and using mean squared error, this network outperformed the Black- Scholes model in about half the test periods . Other early works continued to use a single hidden layer with up to 11 neurons.
Mezofi and Szabo characterize the final outputs of these neural network approaches in three categories: directly predicting the option premium, while possibly adding in implied volatility or other engineered features; predicting implied volatility and using it as an input to Black-Scholes to return the option premium ; and finding the ratio between option premium and strike price. Garcia et al introduced the “homogeneity hint" to constrain the set of possible outputs such that the option pricing function is homogeneous in asset price and strike price with degree. Considering option pricing and volatility estimation as a supervised learning problem, the Multi-Layer Perceptron (MLP) has been the workhorse neural network. Recent work has been focused around increasing the complexity of MLPs, up to 4 hidden layers with 400 each . Ensemble and modular neural networks have also combined the outputs of independent MLPs to improve generalization. Although Recurrent Neural Networks (RNNs) have been extensively applied to the stock
price prediction problem, little work exists for volatility estimation and no work exists for options pricing.
